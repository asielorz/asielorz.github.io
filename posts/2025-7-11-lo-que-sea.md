---
title: Todos somos el público de "lo que sea" a veces
date: 2025-7-11
tags:
  sociedad
---
En un reciente artículo titulado [_The rise of Whatever_](https://eev.ee/blog/2025/07/03/the-rise-of-whatever/), la bloguera conocida como Eevee argumenta que las redes neuronales generativas (modelos grandes de lenguaje, modelos de difusión que generan imágenes...) son especialmente buenas haciendo "lo que sea" (_whatever_), es decir, contenido sin intención que cumple la función de rellenar un espacio porque algo tiene que haber. Eevee explica que esto tiene sentido en el modelo de negocio del internet actual, basado en la reproducción de anuncios y la recolección de datos, que necesita maximizar la cantidad de visitas a una página pero no necesariamente la calidad o utilidad de esas visitas. Desde el punto de vista del negocio, el contenido de la página puede ser lo que sea siempre y cuando sea suficiente para atraer el tráfico necesario para generar reproducciones de anuncios y datos de uso. De un tiempo a esta parte me interesa bastante entender por qué la gente usa y quiere usar herramientas como ChatGPT, porque me parece que desestimarlas en su totalidad por sus defectos ignora que millones de personas las están usando y por lo general no me gustan los argumentos que parten de que uno es muy listo y los demás muy tontos. En enero analizábamos [el aspecto de la usabilidad](https://asielorz.github.io/posts/modelos-de-lenguaje-grandes-experiencia-usuario) como uno de los factores fundamentales de estas herramientas. Hoy vamos a meternos de lleno en el berenjenal del contenido. ¿Por qué querríamos usar una herramienta que nos miente y nos desvía con información superficial y poco relevante?

Es difícil tener criterio sobre un tema. Requiere mucho conocimiento y experiencia. La mayoría de la gente no tenemos criterio sobre la mayoría de los temas. No hay tiempo físico en la vida para aprender suficientes cosas sobre todos los temas como para tener un criterio. Todos tenemos que elegir, conscientemente o no, y necesariamente va a resultar que va a haber muchísimos temas sobre los que no sabemos suficiente como para poder discernir. Más que aquellos en los que sí. Esto significa varias cosas. La mayoría de preguntas que se le van a hacer a ChatGPT, por estadística pura, van a ser preguntas superficiales que tienen respuestas enteras en el propio corpus de entrenamiento del modelo y que éste puede recitar palabra por palabra. Que ChatGPT sea bastante malo a la hora de contestar preguntas difíciles es en realidad un problema en una minoría de casos de una minoría de usuarios. También significa que el estilo verboso de estos programas, con tendencia a sobreexcplicar conceptos introductorios, es en realidad una ventaja en la mayoría de los casos, ya que el público más probable necesita esa explicación. También significa que para la mayoría de preguntas, la persona escribiendo la pregunta no tiene la capacidad de discernir si la respuesta es verdad o no, incluso si es una soberana estupidez fácilmente detectable por alguien mínimamente versado en el tema. Esto hace que la reputación de mentiroso de estos programas no sea generalizada porque la mayoría de mentiras, errores factuales y falacias pasan desapercibidas. Todos, en la mayoría de temas, nos conformamos con "lo que sea" porque no somos capaces de distinguirlo de algo mejor.

Quiero hacer especial insistencia en que esto no es un intento de dividir a las personas en una minoría de listos y una mayoría de tontos. Todos tenemos un criterio desarrollado para algunos temas y carecemos de él para los demás. Es una consecuencia natural de la inmensidad del conocimiento y la falta de tiempo. Saber de algo es costoso así que toca elegir. Esto no va de echarle la bronca a nadie. Cuando vemos a alguien tragarse una mentira, ya sea de CharGPT o de producción humana, en un tema del que sabemos mucho, es tentador pensar que estamos por encima de eso sin darnos cuenta de todos los demás temas de los que no sabemos y que nos dan igual y en los que seríamos nosotros los que caeríamos en algo así.

La evidencia también nos dice que hay muchísima demanda en el mundo para "lo que sea". Algunas de las aplicaciones más exitosas del mundo, tanto en número de usuarios como en horas de uso, son scrolls infinitos de "lo que sea". Estoy hablando de las redes sociales de contenido recomendado automáticamente como TikTok, los reels de Instagram, Youtube Shorts o la feed para ti de Twitter.

Los modelos grandes de lenguaje crean la ilusión de tener acceso a conocimientos ilimitados sobre todos los temas. Lo que producen es "lo que sea", pero sin la capacidad de discernir podemos tragarnos ese "lo que sea" creyendo que es verdadero, riguroso y relevante. La mayoría del uso de estas herramientas es superficial y frívolo: mirar curiosidades, propuestas de ideas, búsqueda de información práctica del día a día... Son áreas en las que la tolerancia al error es muy alta y en las que "lo que sea" es a menudo una respuesta suficiente. Los malentendidos y desinformaciones, cosas como ChatGPT me ha dicho que este libro existe pero en la biblioteca me dicen que no o ChatGPT me ha dicho que esta tienda está abierta pero he ido y me la he encontrado cerrada dan situaciones más anecdóticas que catastróficas. Desde este punto de vista, satisfacen mayormente la demanda de sus usuarios. Recalco explícitamente que el recibir información verdadera no es parte de la demanda en cuestión.

El gran problema por supuesto viene cuando se usan para cosas serias, sin el criterio para juzgar las respuestas. Tenemos ya varios ejemplos en la prensa de resultados catastróficos tras usar modelos grandes de lenguaje como psicólogos, como explican [este artículo en The Independent](https://www.independent.co.uk/tech/chatgpt-ai-therapy-chatbot-psychosis-mental-health-b2784454.html) y [este otro en Rolling Stone](https://www.rollingstone.com/culture/culture-features/ai-spiritual-delusions-destroying-human-relationships-1235330175/). Se podría argumentar también que el usar estas herramientas para temas más banales puede llevar a normalizar su uso en la mente del usuario y servir de puerta de entrada a usos más problemáticos como los arriba mencionados.

Entonces, ¿está bien que los modelos grandes de lenguaje mientan? Al parecer eso es suficiente, es lo que quieren sus usuarios. Aquí la respuesta es que evidentemente no. Podemos tener objeciones éticas y también pragmáticas perfectamente válidas contra una máquina que miente a escala a una gran parte de la población del planeta sin que sean capaces de darse cuenta. La información que recibimos debería ser verdad siempre, aunque en ese caso concreto creamos que nos conformaríamos con menos. Lo que intenta este texto es entender la motivación de los usuarios. Responder a la pregunta: si esta máquina miente, y sabemos que lo hace y lo volverá a hacer, ¿por qué se sigue usando? Espero que haya contribuido a esclarecer algo. Otra conclusión que podemos sacar es que la estrategia de explicar que los modelos grandes de lenguaje no saben cosas y mienten o alucinan no es efectiva, porque no contrarresta las razones por las que son usados. Es decir, no vas a convencer al usuario de ChatGPT de que no lo use porque miente si al usuario de ChatGPT no le importa que mienta.

ChatGPT es una máquina muy eficiente de producir "lo que sea". Coincidentemente, resulta que ahora mismo en el mundo "lo que sea" es un producto ampliamente demandado.
