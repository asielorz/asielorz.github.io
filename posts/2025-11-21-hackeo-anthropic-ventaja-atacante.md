---
title: El hackeo de Anthropic y la ventaja del atacante
date: 2025-11-21
image: /images/matrix.png
image-alt: Imagen de stock de letras verdes cayendo sobre fondo negro como en la película Matrix.
image-credit: "Fotografía de stock de Markus Spiske ([fuente](https://www.pexels.com/es-es/foto/software-matriz-codigos-1089438/))"
tags:
  informática
  sociedad
---
Una [reciente publicación del blog de Anthropic](https://www.anthropic.com/news/disrupting-AI-espionage) explica cómo se han usado modelos de la empresa para llevar a cabo campañas de espionaje por un grupo presuntamente vinculado al gobierno chino. El artículo es muy interesante tanto por lo que dice como por lo que no, así que vamos a desgranarlo.

Por cómo describen el ataque, refiriéndose a los "agentes" de Anthropic como "sistemas que pueden funcionar de manera autónoma durante periodos largos de tiempo y completar tareas complejas en gran medida independientemente de la intervención humana", parecería que este grupo de espías ha montado una granja de servidores que está hackeando a gente por el mundo de manera automática sin necesidad de intervención humana. Una persona leyendo el artículo por encima o quedándose en la introducción se llevaría esta imagen. A continuación dan una explicación paso a paso del ataque y vemos que más bien no. El proceso tenía numerosos puntos en los que un humano recibía los resultados de la máquina, los evaluaba y tomaba decisiones al respecto. La herramienta es útil a la hora de automatizar algunos pasos pero no es capaz de llevar a cabo todo el proceso de manera fiable. Esto encaja con el estado actual de esta tecnología y con [un reciente estudio de la universidad Carnegie Mellon](https://www.theregister.com/2025/06/29/ai_agents_fail_a_lot/) que dice que los agentes fallan un 70% de las veces a la hora de llevar a cabo tareas comunes de oficina.

La conclusión del artículo de Anthropic es igualmente engañosa. Se hacen la siguente pregunta: "¿si los modelos de IA pueden ser usados para ciberataques a esta escala, por qué continuar desarrollándolos y publicándolos?" a lo que responden que las mismas habilidades que permiten usar a Claude para hackear son las que permiten usarlo para prevenir estos ataques. No es sorprendente que el blog de la empresa que la empresa que desarrolla y vende estos modelos dedique su blog a hacer publicidad de los mismos pero desde fuera cabría preguntarse hasta qué punto es verdad esta afirmación.

La ventaja del atacante es el nombre que se le da al siguiente fenómeno. En un enfrentamiento asimétrico como es el hackeo la condición de éxito de ambos bandos se mide de maneras muy distintas. Un sistema tiene que ser seguro todos los días en todos sus frentes para evitar filtrar datos personales o ser controlado remotamente mientras que el atacante sólo tiene que encontrar y explotar una vulnerabilidad una vez para lograr acceso a todos esos datos. Esa es la ventaja del atacante: el defensor tiene que tener suerte siempre, el atacante sólo necesita tener suerte una vez.

Esto significa que las operaciones que llevan a cabo y los sistemas que usan unos y otros son muy distintos. El defensor necesita de fiabilidad y consistencia para poder asegurar la seguridad a lo largo del tiempo. Hoy en día a menudo esto se logra mediante sistemas totalmente automatizados muy robustos y fiables que están operativos las 24 horas del día y permiten reducir la cantidad de supervisión humana necesaria, ya que esta es cara y falible. Gran parte de los recursos se destinan a la prevención y a la seguridad pasiva porque lo contrario sería demasiado caro y tendría demasiados falsos positivos. La mayor parte del tiempo no hay nadie atacando. Los ataques sin embargo suelen ser operaciones puntuales que buscan tener éxito una sola vez. Pueden permitirse la intervención humana y el dedicar muchos recursos a una operación puntual porque la ganancia potencial en caso de éxito es muy grande. La consistencia y la fiabilidad son atributos menos relevantes porque no hace falta sostener nada en el tiempo y los errores pueden ser detectados y corregidos mediante supervisión humana.

La inteligencia artificial generativa ha demostrado que si algo no es, es consistente. A veces es capaz de sacar grandes cantidades de trabajo en muy poco tiempo y a menudo es capaz de fracasar estrepitosamente de la forma más insospechada. Como mencionábamos antes, los agentes fallan en un 70% de las tareas. Además de eso, los grandes modelos de lenguaje [escriben código con más vulnerabilidades que los humanos](https://www.techradar.com/pro/nearly-half-of-all-code-generated-by-ai-found-to-contain-security-flaws-even-big-llms-affected), se inventan paquetes dando lugar a un nuevo tipo de vulnerabilidad conocido como [_slopsquatting_](https://en.wikipedia.org/wiki/Slopsquatting), [alucinan respuestas](https://es.wikipedia.org/wiki/Alucinaci%C3%B3n_(inteligencia_artificial)) o [se inventan bibliografía](https://studyfinds.org/chatgpts-hallucination-problem-fabricated-references/). Esto la hace nefasta para el defensor, que como explicábamos antes valora ante todo la fiabilidad y la consistencia. Sin embargo y como ta hemos visto en el caso que abre este texto es una herramienta excelente para el atacante ya que permite ahorrar y simplificar grandes cantidades de trabajo y los errores pueden ser mitigados por el operador humano que dirige el ataque.

Desde Anthropic afirman que su tecnología será la solución a los problemas que ella misma está causando pero cabe preguntarse la veracidad de esta afirmación y de momento la hemeroteca no está de su lado.
